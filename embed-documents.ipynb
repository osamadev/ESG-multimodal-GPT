{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_27656\\2959736974.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_documents(pdf_folder_path: str):\n",
    "    documents = []\n",
    "    file_Ids = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=100, \n",
    "        length_function=len, \n",
    "        is_separator_regex=False,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    return chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_pdf_documents('./new_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1468 document(s) in your data\n",
      "There are 645 characters in your document\n"
     ]
    }
   ],
   "source": [
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[30].page_content)} characters in your document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "OPENAI_API_KEY =os.environ[\"OPENAI_API_KEY\"]\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.environ[\"PINECONE_API_KEY\"],\n",
    "    environment=os.environ[\"PINECONE_ENV\"]\n",
    ")\n",
    "index_name = \"esg-index\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name not in pinecone.list_indexes():\n",
    "    # we create a new index\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='cosine',\n",
    "        dimension=1536  # 1536 dim of text-embedding-ada-002\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.1,\n",
       " 'namespaces': {'': {'vector_count': 8757}},\n",
       " 'total_vector_count': 8757}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.GRPCIndex(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca945d96646f4f35b1a31f5b162e6ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    # get end of batch\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    batch = data[i:i_end]\n",
    "\n",
    "    # first get metadata fields for this record\n",
    "    metadatas = [{\n",
    "        'text': record.page_content\n",
    "    } for _, record in enumerate(batch)]\n",
    "    # get the list of contexts / documents\n",
    "    documents = batch\n",
    "    # create document embeddings\n",
    "\n",
    "    embeds = embed.embed_documents([str(doc.page_content) for doc in documents])\n",
    "    # get IDs\n",
    "    # Create IDs for each chunk\n",
    "    ids = [uuid4().hex for _ in range(len(embeds))]\n",
    "    # add everything to pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.delete(delete_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# Define a function to create embeddings\n",
    "def create_embeddings(texts):\n",
    "    embeddings_list = []\n",
    "    file_ids = []\n",
    "    for text in texts:\n",
    "        res = openai.Embedding.create(input=[text], engine=MODEL)\n",
    "        embeddings_list.append(res['data'][0]['embedding'])\n",
    "        file_ids.append()\n",
    "    return embeddings_list\n",
    "\n",
    "# Define a function to upsert embeddings to Pinecone\n",
    "def upsert_embeddings_to_pinecone(index, embeddings, ids):\n",
    "    index.upsert(vectors=[(id, embedding) for id, embedding in zip(ids, embeddings)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import openai\n",
    "\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize OpenAI\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Replace consecutive spaces, newlines and tabs\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def process_pdf(file_path):\n",
    "    # create a loader\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    # load your data\n",
    "    data = loader.load()\n",
    "    # Split your data up into smaller documents with Chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(        \n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=100, \n",
    "        length_function=len, \n",
    "        is_separator_regex=False,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "    documents = text_splitter.split_documents(data)\n",
    "    # Convert Document objects into strings\n",
    "    texts = [doc for doc in documents]\n",
    "    return texts\n",
    "\n",
    "# Define a function to upsert embeddings to Pinecone\n",
    "def upsert_embeddings_to_pinecone(index, embeddings, ids):\n",
    "    index.upsert(vectors=[(id, embedding) for id, embedding in zip(ids, embeddings)])\n",
    "\n",
    "pdf_folder_path = './dataset'\n",
    "\n",
    "for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            file_path = os.path.join(pdf_folder_path, file)\n",
    "            # Process a PDF and create embeddings\n",
    "            texts = process_pdf(file_path)\n",
    "            # create document embeddings\n",
    "            embeds = embed.embed_documents(texts)\n",
    "\n",
    "            # Upsert the embeddings to Pinecone\n",
    "            upsert_embeddings_to_pinecone(index, embeds, [file_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2752\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            file_path = os.path.join(pdf_folder_path, file)\n",
    "            # Process a PDF and create embeddings\n",
    "            texts = process_pdf(file_path)\n",
    "            total += len(texts)\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb299c42f12c4da49d97490b17a50f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_27656\\1324469478.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch_df['context_chunks'] = batch_df['context'].apply(lambda x: text_splitter.split_documents([x]))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m data \u001b[38;5;241m=\u001b[39m process_pdf_documents(pdf_folder_path)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Assuming 'embed' is your embedding model and 'index' is your Pinecone index\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[43msplit_and_embed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 64\u001b[0m, in \u001b[0;36msplit_and_embed_documents\u001b[1;34m(dataframe, embed, index)\u001b[0m\n\u001b[0;32m     61\u001b[0m ids \u001b[38;5;241m=\u001b[39m [uuid4() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(embeds))]\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Upsert to Pinecone\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\usama\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\pinecone\\core\\grpc\\index_grpc.py:421\u001b[0m, in \u001b[0;36mGRPCIndex.upsert\u001b[1;34m(self, vectors, async_req, namespace, batch_size, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid vector value passed: cannot interpret type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    419\u001b[0m timeout \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 421\u001b[0m vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_vector_transform, vectors))\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m async_req:\n\u001b[0;32m    423\u001b[0m     args_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_non_empty_args([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace\u001b[39m\u001b[38;5;124m'\u001b[39m, namespace)])\n",
      "File \u001b[1;32mc:\\Users\\usama\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\pinecone\\core\\grpc\\index_grpc.py:414\u001b[0m, in \u001b[0;36mGRPCIndex.upsert.<locals>._vector_transform\u001b[1;34m(item)\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound a tuple of length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is not supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m    411\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectors can be represented as tuples either the form (id, values, metadata) or (id, values). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo pass sparse values please use either dicts or GRPCVector objects as inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28mid\u001b[39m, values, metadata \u001b[38;5;241m=\u001b[39m fix_tuple_length(item, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GRPCVector(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m, values\u001b[38;5;241m=\u001b[39mvalues, metadata\u001b[38;5;241m=\u001b[39m\u001b[43mdict_to_proto_struct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, Mapping):\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _dict_to_grpc_vector(item)\n",
      "File \u001b[1;32mc:\\Users\\usama\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\pinecone\\core\\utils\\__init__.py:83\u001b[0m, in \u001b[0;36mdict_to_proto_struct\u001b[1;34m(d)\u001b[0m\n\u001b[0;32m     81\u001b[0m     d \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     82\u001b[0m s \u001b[38;5;241m=\u001b[39m Struct()\n\u001b[1;32m---> 83\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "File \u001b[1;32mc:\\Users\\usama\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\google\\protobuf\\internal\\well_known_types.py:820\u001b[0m, in \u001b[0;36mStruct.update\u001b[1;34m(self, dictionary)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, dictionary):  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n\u001b[0;32m    819\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictionary\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 820\u001b[0m     \u001b[43m_SetStructValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\usama\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\google\\protobuf\\internal\\well_known_types.py:751\u001b[0m, in \u001b[0;36m_SetStructValue\u001b[1;34m(struct_value, value)\u001b[0m\n\u001b[0;32m    749\u001b[0m   struct_value\u001b[38;5;241m.\u001b[39mlist_value\u001b[38;5;241m.\u001b[39mextend(value)\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected type\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected type"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from uuid import uuid4\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def process_pdf_documents(pdf_folder_path: str):\n",
    "    records = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            loaded_docs = loader.load()\n",
    "\n",
    "            for doc in loaded_docs:\n",
    "                records.append({\n",
    "                    'title': os.path.splitext(file)[0],  # File name without extension as title\n",
    "                    'context': doc,  # Storing the document content in 'context',\n",
    "                    'file_name': file\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def split_and_embed_documents(dataframe, embed, index):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=100, \n",
    "        length_function=len, \n",
    "        is_separator_regex=False,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    batch_size = 100\n",
    "    for i in tqdm(range(0, len(dataframe), batch_size)):\n",
    "        batch_df = dataframe.iloc[i:i + batch_size]\n",
    "\n",
    "        # first get metadata fields for this record\n",
    "        metadatas = [{\n",
    "            'title': record['title'],\n",
    "            'text': record['context'],\n",
    "            'file_name': record['file_name']\n",
    "            } for _, record in batch_df.iterrows()]\n",
    "        \n",
    "        # Splitting the documents into chunks\n",
    "        batch_df['context_chunks'] = batch_df['context'].apply(lambda x: text_splitter.split_documents([x]))\n",
    "\n",
    "        # Flatten the context for embedding\n",
    "        flattened_contexts = batch_df.explode('context_chunks')['context_chunks'].tolist()\n",
    "\n",
    "        # Ensure each element in flattened_contexts is a string\n",
    "        flattened_contexts = [str(context) for context in flattened_contexts if context]\n",
    "\n",
    "        # Check if embed_documents expects a list or single strings\n",
    "        try:\n",
    "            # If it expects a list\n",
    "            embeds = embed.embed_documents(flattened_contexts)\n",
    "        except TypeError:\n",
    "            # If it expects individual strings\n",
    "            embeds = [embed.embed_documents(context) for context in flattened_contexts]\n",
    "\n",
    "        # Create IDs for each chunk\n",
    "        ids = [uuid4() for _ in range(len(embeds))]\n",
    "\n",
    "        # Upsert to Pinecone\n",
    "        index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "\n",
    "# Usage\n",
    "pdf_folder_path = './dataset/'\n",
    "data = process_pdf_documents(pdf_folder_path)\n",
    "\n",
    "# Assuming 'embed' is your embedding model and 'index' is your Pinecone index\n",
    "split_and_embed_documents(data, embed, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.1,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader([\"https://www.sustainalytics.com/esg-research\", \"https://www.globalreporting.org/\"])\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
