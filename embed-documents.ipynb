{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_27656\\2959736974.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_documents(pdf_folder_path: str):\n",
    "    documents = []\n",
    "    file_Ids = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=100, \n",
    "        length_function=len, \n",
    "        is_separator_regex=False,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    return chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_pdf_documents('./new_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1468 document(s) in your data\n",
      "There are 645 characters in your document\n"
     ]
    }
   ],
   "source": [
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[30].page_content)} characters in your document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "OPENAI_API_KEY =os.environ[\"OPENAI_API_KEY\"]\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.environ[\"PINECONE_API_KEY\"],\n",
    "    environment=os.environ[\"PINECONE_ENV\"]\n",
    ")\n",
    "index_name = \"esg-index\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name not in pinecone.list_indexes():\n",
    "    # we create a new index\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='cosine',\n",
    "        dimension=1536  # 1536 dim of text-embedding-ada-002\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.1,\n",
       " 'namespaces': {'': {'vector_count': 8757}},\n",
       " 'total_vector_count': 8757}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.GRPCIndex(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca945d96646f4f35b1a31f5b162e6ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    # get end of batch\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    batch = data[i:i_end]\n",
    "\n",
    "    # first get metadata fields for this record\n",
    "    metadatas = [{\n",
    "        'text': record.page_content\n",
    "    } for _, record in enumerate(batch)]\n",
    "    # get the list of contexts / documents\n",
    "    documents = batch\n",
    "    # create document embeddings\n",
    "\n",
    "    embeds = embed.embed_documents([str(doc.page_content) for doc in documents])\n",
    "    # get IDs\n",
    "    # Create IDs for each chunk\n",
    "    ids = [uuid4().hex for _ in range(len(embeds))]\n",
    "    # add everything to pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.delete(delete_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# Define a function to create embeddings\n",
    "def create_embeddings(texts):\n",
    "    embeddings_list = []\n",
    "    file_ids = []\n",
    "    for text in texts:\n",
    "        res = openai.Embedding.create(input=[text], engine=MODEL)\n",
    "        embeddings_list.append(res['data'][0]['embedding'])\n",
    "        file_ids.append()\n",
    "    return embeddings_list\n",
    "\n",
    "# Define a function to upsert embeddings to Pinecone\n",
    "def upsert_embeddings_to_pinecone(index, embeddings, ids):\n",
    "    index.upsert(vectors=[(id, embedding) for id, embedding in zip(ids, embeddings)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import openai\n",
    "\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize OpenAI\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Replace consecutive spaces, newlines and tabs\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def process_pdf(file_path):\n",
    "    # create a loader\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    # load your data\n",
    "    data = loader.load()\n",
    "    # Split your data up into smaller documents with Chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(        \n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=100, \n",
    "        length_function=len, \n",
    "        is_separator_regex=False,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "    documents = text_splitter.split_documents(data)\n",
    "    # Convert Document objects into strings\n",
    "    texts = [doc for doc in documents]\n",
    "    return texts\n",
    "\n",
    "# Define a function to upsert embeddings to Pinecone\n",
    "def upsert_embeddings_to_pinecone(index, embeddings, ids):\n",
    "    index.upsert(vectors=[(id, embedding) for id, embedding in zip(ids, embeddings)])\n",
    "\n",
    "pdf_folder_path = './dataset'\n",
    "\n",
    "for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            file_path = os.path.join(pdf_folder_path, file)\n",
    "            # Process a PDF and create embeddings\n",
    "            texts = process_pdf(file_path)\n",
    "            # create document embeddings\n",
    "            embeds = embed.embed_documents(texts)\n",
    "\n",
    "            # Upsert the embeddings to Pinecone\n",
    "            upsert_embeddings_to_pinecone(index, embeds, [file_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2752\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            file_path = os.path.join(pdf_folder_path, file)\n",
    "            # Process a PDF and create embeddings\n",
    "            texts = process_pdf(file_path)\n",
    "            total += len(texts)\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from uuid import uuid4\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def process_pdf_documents(pdf_folder_path: str):\n",
    "    records = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            loaded_docs = loader.load()\n",
    "\n",
    "            for doc in loaded_docs:\n",
    "                records.append({\n",
    "                    'title': os.path.splitext(file)[0],  # File name without extension as title\n",
    "                    'context': doc,  # Storing the document content in 'context',\n",
    "                    'file_name': file\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def split_and_embed_documents(dataframe, embed, index):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=100, \n",
    "        length_function=len, \n",
    "        is_separator_regex=False,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    batch_size = 100\n",
    "    for i in tqdm(range(0, len(dataframe), batch_size)):\n",
    "        batch_df = dataframe.iloc[i:i + batch_size]\n",
    "\n",
    "        # first get metadata fields for this record\n",
    "        metadatas = [{\n",
    "            'title': record['title'],\n",
    "            'text': record['context'],\n",
    "            'file_name': record['file_name']\n",
    "            } for _, record in batch_df.iterrows()]\n",
    "        \n",
    "        # Splitting the documents into chunks\n",
    "        batch_df['context_chunks'] = batch_df['context'].apply(lambda x: text_splitter.split_documents([x]))\n",
    "\n",
    "        # Flatten the context for embedding\n",
    "        flattened_contexts = batch_df.explode('context_chunks')['context_chunks'].tolist()\n",
    "\n",
    "        # Ensure each element in flattened_contexts is a string\n",
    "        flattened_contexts = [str(context) for context in flattened_contexts if context]\n",
    "\n",
    "        # Check if embed_documents expects a list or single strings\n",
    "        try:\n",
    "            # If it expects a list\n",
    "            embeds = embed.embed_documents(flattened_contexts)\n",
    "        except TypeError:\n",
    "            # If it expects individual strings\n",
    "            embeds = [embed.embed_documents(context) for context in flattened_contexts]\n",
    "\n",
    "        # Create IDs for each chunk\n",
    "        ids = [uuid4() for _ in range(len(embeds))]\n",
    "\n",
    "        # Upsert to Pinecone\n",
    "        index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "\n",
    "# Usage\n",
    "pdf_folder_path = './dataset/'\n",
    "data = process_pdf_documents(pdf_folder_path)\n",
    "\n",
    "# Assuming 'embed' is your embedding model and 'index' is your Pinecone index\n",
    "split_and_embed_documents(data, embed, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.1,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader([\"\", \"\"])\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
